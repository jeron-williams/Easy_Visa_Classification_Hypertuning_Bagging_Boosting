{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jeron-williams/Easy_Visa_Classification_Hypertuning_Bagging_Boosting/blob/main/Copy_of_EasyVisa_Full_Code_Notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yZvo8CHcetWN",
      "metadata": {
        "id": "yZvo8CHcetWN"
      },
      "source": [
        "## Problem Statement"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "empty-shanghai",
      "metadata": {
        "id": "empty-shanghai"
      },
      "source": [
        "### Business Context"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wJmgAFK8_aCN",
      "metadata": {
        "id": "wJmgAFK8_aCN"
      },
      "source": [
        "Business communities in the United States are facing high demand for human resources, but one of the constant challenges is identifying and attracting the right talent, which is perhaps the most important element in remaining competitive. Companies in the United States look for hard-working, talented, and qualified individuals both locally as well as abroad.\n",
        "\n",
        "The Immigration and Nationality Act (INA) of the US permits foreign workers to come to the United States to work on either a temporary or permanent basis. The act also protects US workers against adverse impacts on their wages or working conditions by ensuring US employers' compliance with statutory requirements when they hire foreign workers to fill workforce shortages. The immigration programs are administered by the Office of Foreign Labor Certification (OFLC).\n",
        "\n",
        "OFLC processes job certification applications for employers seeking to bring foreign workers into the United States and grants certifications in those cases where employers can demonstrate that there are not sufficient US workers available to perform the work at wages that meet or exceed the wage paid for the occupation in the area of intended employment."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3CWyKzKYAfEp",
      "metadata": {
        "id": "3CWyKzKYAfEp"
      },
      "source": [
        "### Objective"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cdq4cea5Akh8",
      "metadata": {
        "id": "cdq4cea5Akh8"
      },
      "source": [
        "In FY 2016, the OFLC processed 775,979 employer applications for 1,699,957 positions for temporary and permanent labor certifications. This was a nine percent increase in the overall number of processed applications from the previous year. The process of reviewing every case is becoming a tedious task as the number of applicants is increasing every year.\n",
        "\n",
        "The increasing number of applicants every year calls for a Machine Learning based solution that can help in shortlisting the candidates having higher chances of VISA approval. OFLC has hired the firm EasyVisa for data-driven solutions. You as a data  scientist at EasyVisa have to analyze the data provided and, with the help of a classification model:\n",
        "\n",
        "* Facilitate the process of visa approvals.\n",
        "* Recommend a suitable profile for the applicants for whom the visa should be certified or denied based on the drivers that significantly influence the case status."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LpNqj6EzAhxy",
      "metadata": {
        "id": "LpNqj6EzAhxy"
      },
      "source": [
        "### Data Description"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uae5fMIyAnd-",
      "metadata": {
        "id": "uae5fMIyAnd-"
      },
      "source": [
        "The data contains the different attributes of employee and the employer. The detailed data dictionary is given below.\n",
        "\n",
        "* case_id: ID of each visa application\n",
        "* continent: Information of continent the employee\n",
        "* education_of_employee: Information of education of the employee\n",
        "* has_job_experience: Does the employee has any job experience? Y= Yes; N = No\n",
        "* requires_job_training: Does the employee require any job training? Y = Yes; N = No\n",
        "* no_of_employees: Number of employees in the employer's company\n",
        "* yr_of_estab: Year in which the employer's company was established\n",
        "* region_of_employment: Information of foreign worker's intended region of employment in the US.\n",
        "* prevailing_wage:  Average wage paid to similarly employed workers in a specific occupation in the area of intended employment. The purpose of the prevailing wage is to ensure that the foreign worker is not underpaid compared to other workers offering the same or similar service in the same area of employment.\n",
        "* unit_of_wage: Unit of prevailing wage. Values include Hourly, Weekly, Monthly, and Yearly.\n",
        "* full_time_position: Is the position of work full-time? Y = Full Time Position; N = Part Time Position\n",
        "* case_status:  Flag indicating if the Visa was certified or denied"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Lm7obbsV_RUT",
      "metadata": {
        "id": "Lm7obbsV_RUT"
      },
      "source": [
        "## Installing and Importing the necessary libraries"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OS2VAv465IZa",
      "metadata": {
        "id": "OS2VAv465IZa"
      },
      "source": [
        "**Note**: *After running the above cell, kindly restart the notebook kernel and run all cells sequentially from the below.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "canadian-maple",
      "metadata": {
        "id": "canadian-maple"
      },
      "outputs": [],
      "source": [
        "# Standard library utils\n",
        "import time\n",
        "import warnings\n",
        "\n",
        "# Data wrangling\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "# Model selection / CV\n",
        "from sklearn.model_selection import (\n",
        "    train_test_split,\n",
        "    StratifiedKFold,    # preferred for classification\n",
        "    GridSearchCV,\n",
        "    RandomizedSearchCV,\n",
        "    cross_val_score,\n",
        "    cross_validate,\n",
        ")\n",
        "\n",
        "# Metrics\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score,\n",
        "    precision_score,\n",
        "    recall_score,\n",
        "    f1_score,\n",
        "    confusion_matrix,\n",
        "    classification_report,\n",
        "    make_scorer,\n",
        ")\n",
        "\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from collections import Counter\n",
        "\n",
        "# Preprocessing\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Models\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.ensemble import (\n",
        "    BaggingClassifier,\n",
        "    RandomForestClassifier,\n",
        "    GradientBoostingClassifier,\n",
        "    AdaBoostClassifier,\n",
        ")\n",
        "\n",
        "# Imbalanced learning\n",
        "from imblearn.over_sampling import SMOTE\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "from imblearn.pipeline import Pipeline as ImbPipeline\n",
        "\n",
        "# XGBoost (optional)\n",
        "try:\n",
        "    from xgboost import XGBClassifier\n",
        "    HAS_XGB = True\n",
        "except Exception:\n",
        "    HAS_XGB = False\n",
        "\n",
        "# Global config\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "sns.set(style=\"whitegrid\", context=\"notebook\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "thorough-passion",
      "metadata": {
        "id": "thorough-passion"
      },
      "source": [
        "## Import Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "q_MqHR8tN8mz",
      "metadata": {
        "id": "q_MqHR8tN8mz"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "data= pd.read_csv(\"/content/drive/My Drive/Advanced Machine Learning Module 3/EasyVisa.csv\")\n",
        "\n",
        "copydata = data.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DLPXJYu2quWk",
      "metadata": {
        "id": "DLPXJYu2quWk"
      },
      "outputs": [],
      "source": [
        "# Convert notebook -> HTML in the same Drive folder\n",
        "!jupyter nbconvert --to html \"/content/drive/MyDrive/Advanced Machine Learning Module 3/EasyVisa_Full_Code_Notebook.ipynb\"\n",
        "\n",
        "# Download the HTML that was just created\n",
        "from google.colab import files\n",
        "files.download(\"/content/drive/MyDrive/Advanced Machine Learning Module 3/EasyVisa_Full_Code_Notebook.html\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mq-1s9p-_aKl",
      "metadata": {
        "id": "mq-1s9p-_aKl"
      },
      "source": [
        "## Overview of the Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aboriginal-wrist",
      "metadata": {
        "id": "aboriginal-wrist"
      },
      "source": [
        "#### View the first and last 5 rows of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qvj7EJmfcms5",
      "metadata": {
        "id": "qvj7EJmfcms5"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cVzRbi7oN6br",
      "metadata": {
        "id": "cVzRbi7oN6br"
      },
      "outputs": [],
      "source": [
        "# First 5 rows (default)\n",
        "copydata.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accessory-camel",
      "metadata": {
        "id": "accessory-camel"
      },
      "source": [
        "#### Understand the shape of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ym8ApC21N64n",
      "metadata": {
        "id": "Ym8ApC21N64n"
      },
      "outputs": [],
      "source": [
        "# (rows, columns)\n",
        "copydata.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "assigned-berkeley",
      "metadata": {
        "id": "assigned-berkeley"
      },
      "source": [
        "#### Check the data types of the columns for the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ekk0QEpXN7im",
      "metadata": {
        "id": "ekk0QEpXN7im"
      },
      "outputs": [],
      "source": [
        "# Schema summary: columns, non-nulls, dtypes, memory\n",
        "copydata.info()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c9-NXH_e42G",
      "metadata": {
        "id": "2c9-NXH_e42G"
      },
      "outputs": [],
      "source": [
        "for feature in copydata.columns: # Loop through all columns in the dataframe\n",
        "    if copydata[feature].dtype == 'object': # Only apply for columns with categorical strings\n",
        "        copydata[feature] = pd.Categorical(copydata[feature])# Replace strings with an integer\n",
        "copydata.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8vRIFMyfM_1",
      "metadata": {
        "id": "c8vRIFMyfM_1"
      },
      "outputs": [],
      "source": [
        "# Column data types\n",
        "print(copydata.dtypes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "MwAfNWGpUk3U",
      "metadata": {
        "id": "MwAfNWGpUk3U"
      },
      "outputs": [],
      "source": [
        "# Missing values per column (counts)\n",
        "copydata.isnull().sum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2BfONSETF5Bn",
      "metadata": {
        "id": "2BfONSETF5Bn"
      },
      "outputs": [],
      "source": [
        "copydata.duplicated().sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "standing-horizontal",
      "metadata": {
        "id": "standing-horizontal"
      },
      "source": [
        "## Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "american-venue",
      "metadata": {
        "id": "american-venue"
      },
      "source": [
        "#### Let's check the statistical summary of the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PsJ9MaHRN4U5",
      "metadata": {
        "id": "PsJ9MaHRN4U5"
      },
      "outputs": [],
      "source": [
        "# Descriptive stats for all dtypes (numeric, object, category, datetime)\n",
        "copydata.describe(include='all')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "competent-timing",
      "metadata": {
        "id": "competent-timing"
      },
      "source": [
        "#### Fixing the negative values in number of employees columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_ltjRQiBN40d",
      "metadata": {
        "id": "_ltjRQiBN40d"
      },
      "outputs": [],
      "source": [
        "# Count negatives and list offending rows\n",
        "print(\"Number of negatives:\", (copydata['no_of_employees'] < 0).sum())\n",
        "\n",
        "negatives = copydata[copydata['no_of_employees'] < 0]\n",
        "print(negatives)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LRaECHlBtEp3",
      "metadata": {
        "id": "LRaECHlBtEp3"
      },
      "outputs": [],
      "source": [
        "# Make values non-negative, then verify none remain negative\n",
        "copydata['no_of_employees'] = copydata['no_of_employees'].abs()\n",
        "print(\"Number of negatives:\", (copydata['no_of_employees'] < 0).sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cutting-bookmark",
      "metadata": {
        "id": "cutting-bookmark"
      },
      "source": [
        "#### Let's check the count of each unique category in each of the categorical variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tjT97Rc9N5SC",
      "metadata": {
        "id": "tjT97Rc9N5SC"
      },
      "outputs": [],
      "source": [
        "# Distinct values per column (NaN excluded)\n",
        "copydata.nunique()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sgEZzUP5b9Va",
      "metadata": {
        "id": "sgEZzUP5b9Va"
      },
      "outputs": [],
      "source": [
        "# Descriptive stats for all dtypes\n",
        "print(copydata.describe(include='all'))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wooden-christian",
      "metadata": {
        "id": "wooden-christian"
      },
      "source": [
        "### Univariate Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "D2X4lDiHa9qF",
      "metadata": {
        "id": "D2X4lDiHa9qF"
      },
      "outputs": [],
      "source": [
        "def histogram_boxplot(data, feature, figsize=(15, 10), kde=False, bins=None):\n",
        "    \"\"\"\n",
        "    Boxplot and histogram combined\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    figsize: size of figure (default (15,10))\n",
        "    kde: whether to show the density curve (default False)\n",
        "    bins: number of bins for histogram (default None)\n",
        "    \"\"\"\n",
        "    f2, (ax_box2, ax_hist2) = plt.subplots(\n",
        "        nrows=2,  # Number of rows of the subplot grid= 2\n",
        "        sharex=True,  # x-axis will be shared among all subplots\n",
        "        gridspec_kw={\"height_ratios\": (0.25, 0.75)},\n",
        "        figsize=figsize,\n",
        "    )  # creating the 2 subplots\n",
        "    sns.boxplot(\n",
        "        data=data, x=feature, ax=ax_box2, showmeans=True, color=\"violet\"\n",
        "    )  # boxplot will be created and a triangle will indicate the mean value of the column\n",
        "    sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2, bins=bins\n",
        "    ) if bins else sns.histplot(\n",
        "        data=data, x=feature, kde=kde, ax=ax_hist2\n",
        "    )  # For histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].mean(), color=\"green\", linestyle=\"--\"\n",
        "    )  # Add mean to the histogram\n",
        "    ax_hist2.axvline(\n",
        "        data[feature].median(), color=\"black\", linestyle=\"-\"\n",
        "    )  # Add median to the histogram\n",
        "\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "R59VWoa5Wdbx",
      "metadata": {
        "id": "R59VWoa5Wdbx"
      },
      "outputs": [],
      "source": [
        "# function to create labeled barplots\n",
        "\n",
        "\n",
        "def labeled_barplot(data, feature, perc=False, n=None):\n",
        "    \"\"\"\n",
        "    Barplot with percentage at the top\n",
        "\n",
        "    data: dataframe\n",
        "    feature: dataframe column\n",
        "    perc: whether to display percentages instead of count (default is False)\n",
        "    n: displays the top n category levels (default is None, i.e., display all levels)\n",
        "    \"\"\"\n",
        "\n",
        "    total = len(data[feature])  # length of the column\n",
        "    count = data[feature].nunique()\n",
        "    if n is None:\n",
        "        plt.figure(figsize=(count + 1, 5))\n",
        "    else:\n",
        "        plt.figure(figsize=(n + 1, 5))\n",
        "\n",
        "    plt.xticks(rotation=90, fontsize=15)\n",
        "    ax = sns.countplot(\n",
        "        data=data,\n",
        "        x=feature,\n",
        "        palette=\"Paired\",\n",
        "        order=data[feature].value_counts().index[:n].sort_values(),\n",
        "    )\n",
        "\n",
        "    for p in ax.patches:\n",
        "        if perc == True:\n",
        "            label = \"{:.1f}%\".format(\n",
        "                100 * p.get_height() / total\n",
        "            )  # percentage of each class of the category\n",
        "        else:\n",
        "            label = p.get_height()  # count of each level of the category\n",
        "\n",
        "        x = p.get_x() + p.get_width() / 2  # width of the plot\n",
        "        y = p.get_height()  # height of the plot\n",
        "\n",
        "        ax.annotate(\n",
        "            label,\n",
        "            (x, y),\n",
        "            ha=\"center\",\n",
        "            va=\"center\",\n",
        "            size=12,\n",
        "            xytext=(0, 5),\n",
        "            textcoords=\"offset points\",\n",
        "        )  # annotate the percentage\n",
        "\n",
        "    plt.show()  # show the plot"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7rsKN6QUZUWM",
      "metadata": {
        "id": "7rsKN6QUZUWM"
      },
      "outputs": [],
      "source": [
        "# Employees: distribution + outliers\n",
        "histogram_boxplot(copydata, 'no_of_employees')\n",
        "\n",
        "# Year established: distribution + outliers\n",
        "histogram_boxplot(copydata, 'yr_of_estab')\n",
        "\n",
        "# Prevailing wage: distribution + outliers (often right-skewed)\n",
        "histogram_boxplot(copydata, 'prevailing_wage')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ilM0XFlYm87K",
      "metadata": {
        "id": "ilM0XFlYm87K"
      },
      "outputs": [],
      "source": [
        "# Labeled bar charts; perc=True shows percentages\n",
        "\n",
        "# Continent distribution (%)\n",
        "labeled_barplot(copydata, 'continent', perc=True)\n",
        "\n",
        "# Education level of employee (%)\n",
        "labeled_barplot(copydata, 'education_of_employee', perc=True)\n",
        "\n",
        "# Has prior job experience (% yes/no)\n",
        "labeled_barplot(copydata, 'has_job_experience', perc=True)\n",
        "\n",
        "# Requires job training (% yes/no)\n",
        "labeled_barplot(copydata, 'requires_job_training', perc=True)\n",
        "\n",
        "# Number of employees (%); bin numeric if many uniques\n",
        "labeled_barplot(copydata, 'no_of_employees', perc=True)\n",
        "\n",
        "# Region of employment (%)\n",
        "labeled_barplot(copydata, 'region_of_employment', perc=True)\n",
        "\n",
        "# Prevailing wage (%); bin/quantile if many uniques\n",
        "labeled_barplot(copydata, 'prevailing_wage', perc=True)\n",
        "\n",
        "# Unit of wage (%)\n",
        "labeled_barplot(copydata, 'unit_of_wage', perc=True)\n",
        "\n",
        "# Full-time vs part-time (%)\n",
        "labeled_barplot(copydata, 'full_time_position', perc=True)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HJo_20Cl1VvN",
      "metadata": {
        "id": "HJo_20Cl1VvN"
      },
      "outputs": [],
      "source": [
        "labeled_barplot(copydata, 'case_status', perc=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "editorial-command",
      "metadata": {
        "id": "editorial-command"
      },
      "source": [
        "#### Observations on education of employee"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d-cdEWZdwpUr",
      "metadata": {
        "id": "d-cdEWZdwpUr"
      },
      "source": [
        "* The highest education in this dataset is a bachelor's degree at 40.2%.\n",
        "* The next highest education is 37.8% at the master's degree level.\n",
        "* 78% of the work visa applications come from people with either a bachelor's degree or a master's degree.\n",
        "* 13.4% of the applicants have a highschool level education\n",
        "* 8.6% have a doctorate level education"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "attempted-burlington",
      "metadata": {
        "id": "attempted-burlington"
      },
      "source": [
        "#### Observations on region of employment"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-wkrXvX9zYgS",
      "metadata": {
        "id": "-wkrXvX9zYgS"
      },
      "source": [
        "* The northeast region has the highest percentage of applicants for work visas at 28.2%\n",
        "* The south region is 27.5%\n",
        "* The west is 25.8%\n",
        "* This could owe to the coastsal closeness of these areas as well as other features like the types of work that require applicants outside the US being more predominant in these regions.\n",
        "* Midwest is 16.9%\n",
        "* Island is 1.5%"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "forbidden-kidney",
      "metadata": {
        "id": "forbidden-kidney"
      },
      "source": [
        "#### Observations on job experience"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qFlf_ViT0wda",
      "metadata": {
        "id": "qFlf_ViT0wda"
      },
      "source": [
        "* 58.1% of the applicants have job experience\n",
        "* 41.9% of the job applicants do not have job experience.\n",
        "* This could be due to the vast majority of applicants without job experience being recent college graduates.\n",
        "* The high level of job experience could be individuals searching for better opportunities or from pursuing extended education and working during the process."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "stunning-surrey",
      "metadata": {
        "id": "stunning-surrey"
      },
      "source": [
        "#### Observations on case status"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "q3O3q8nz1nnN",
      "metadata": {
        "id": "q3O3q8nz1nnN"
      },
      "source": [
        "* 66.8% of the applicants are certified\n",
        "* 33.2% of the applicants are denied\n",
        "* 2/3 of those who apply are certified for work visas."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "equivalent-aging",
      "metadata": {
        "id": "equivalent-aging"
      },
      "source": [
        "### Bivariate Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "blond-convertible",
      "metadata": {
        "id": "blond-convertible"
      },
      "source": [
        "**Creating functions that will help us with further analysis.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adaptive-recipient",
      "metadata": {
        "id": "adaptive-recipient"
      },
      "outputs": [],
      "source": [
        "### function to plot distributions wrt target\n",
        "\n",
        "\n",
        "def distribution_plot_wrt_target(data, predictor, target):\n",
        "\n",
        "    fig, axs = plt.subplots(2, 2, figsize=(12, 10))\n",
        "\n",
        "    target_uniq = data[target].unique()\n",
        "\n",
        "    axs[0, 0].set_title(\"Distribution of target for target=\" + str(target_uniq[0]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[0]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 0],\n",
        "        color=\"teal\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[0, 1].set_title(\"Distribution of target for target=\" + str(target_uniq[1]))\n",
        "    sns.histplot(\n",
        "        data=data[data[target] == target_uniq[1]],\n",
        "        x=predictor,\n",
        "        kde=True,\n",
        "        ax=axs[0, 1],\n",
        "        color=\"orange\",\n",
        "        stat=\"density\",\n",
        "    )\n",
        "\n",
        "    axs[1, 0].set_title(\"Boxplot w.r.t target\")\n",
        "    sns.boxplot(data=data, x=target, y=predictor, ax=axs[1, 0], palette=\"gist_rainbow\")\n",
        "\n",
        "    axs[1, 1].set_title(\"Boxplot (without outliers) w.r.t target\")\n",
        "    sns.boxplot(\n",
        "        data=data,\n",
        "        x=target,\n",
        "        y=predictor,\n",
        "        ax=axs[1, 1],\n",
        "        showfliers=False,\n",
        "        palette=\"gist_rainbow\",\n",
        "    )\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "third-sheriff",
      "metadata": {
        "id": "third-sheriff"
      },
      "outputs": [],
      "source": [
        "def stacked_barplot(data, predictor, target):\n",
        "    \"\"\"\n",
        "    Print the category counts and plot a stacked bar chart\n",
        "\n",
        "    data: dataframe\n",
        "    predictor: independent variable\n",
        "    target: target variable\n",
        "    \"\"\"\n",
        "    count = data[predictor].nunique()\n",
        "    sorter = data[target].value_counts().index[-1]\n",
        "    tab1 = pd.crosstab(data[predictor], data[target], margins=True).sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    print(tab1)\n",
        "    print(\"-\" * 120)\n",
        "    tab = pd.crosstab(data[predictor], data[target], normalize=\"index\").sort_values(\n",
        "        by=sorter, ascending=False\n",
        "    )\n",
        "    tab.plot(kind=\"bar\", stacked=True, figsize=(count + 5, 5))\n",
        "    plt.legend(\n",
        "        loc=\"lower left\", frameon=False,\n",
        "    )\n",
        "    plt.legend(loc=\"upper left\", bbox_to_anchor=(1, 1))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "IafH3N8N8Gq0",
      "metadata": {
        "id": "IafH3N8N8Gq0"
      },
      "outputs": [],
      "source": [
        "# Class-wise distribution: employees vs case_status\n",
        "distribution_plot_wrt_target(copydata, 'no_of_employees', 'case_status')\n",
        "\n",
        "# Class-wise distribution: year established vs case_status\n",
        "distribution_plot_wrt_target(copydata, 'yr_of_estab', 'case_status')\n",
        "\n",
        "# Class-wise distribution: prevailing wage vs case_status\n",
        "distribution_plot_wrt_target(copydata, 'prevailing_wage', 'case_status')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3EYYdF8j8-ig",
      "metadata": {
        "id": "3EYYdF8j8-ig"
      },
      "outputs": [],
      "source": [
        "# Stacked counts by case_status: continent\n",
        "stacked_barplot(copydata, 'continent', 'case_status')\n",
        "\n",
        "# Stacked counts by case_status: education\n",
        "stacked_barplot(copydata, 'education_of_employee', 'case_status')\n",
        "\n",
        "# Stacked counts by case_status: prior job experience\n",
        "stacked_barplot(copydata, 'has_job_experience', 'case_status')\n",
        "\n",
        "# Stacked counts by case_status: requires training\n",
        "stacked_barplot(copydata, 'requires_job_training', 'case_status')\n",
        "\n",
        "# Stacked counts by case_status: employees (bin if many uniques)\n",
        "stacked_barplot(copydata, 'no_of_employees', 'case_status')\n",
        "\n",
        "# Stacked counts by case_status: region\n",
        "stacked_barplot(copydata, 'region_of_employment', 'case_status')\n",
        "\n",
        "# Stacked counts by case_status: wage (bin/quantile for readability)\n",
        "stacked_barplot(copydata, 'prevailing_wage', 'case_status')\n",
        "\n",
        "# Stacked counts by case_status: wage unit\n",
        "stacked_barplot(copydata, 'unit_of_wage', 'case_status')\n",
        "\n",
        "# Stacked counts by case_status: full-time vs part-time\n",
        "stacked_barplot(copydata, 'full_time_position', 'case_status')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kCSzCs48HABT",
      "metadata": {
        "id": "kCSzCs48HABT"
      },
      "outputs": [],
      "source": [
        "# 1) Keep only numeric columns\n",
        "num = copydata.select_dtypes(include=[np.number]).copy()\n",
        "\n",
        "# 2) Optional hygiene: drop mostly-empty and constant columns\n",
        "num = num.dropna(axis=1, thresh=int(0.8 * len(num)))                 # ≥80% non-null\n",
        "const_cols = [c for c in num.columns if num[c].nunique(dropna=True) <= 1]\n",
        "num = num.drop(columns=const_cols)\n",
        "\n",
        "if num.empty:\n",
        "    raise ValueError(\"No numeric columns available after filtering.\")\n",
        "\n",
        "# 3) Correlation + heatmap (lower triangle)\n",
        "corr = num.corr(numeric_only=True)\n",
        "\n",
        "plt.figure(figsize=(12, 8))\n",
        "mask = np.triu(np.ones_like(corr, dtype=bool))\n",
        "sns.heatmap(corr, mask=mask, annot=True, fmt=\".2f\",\n",
        "            cmap=\"coolwarm\", vmin=-1, vmax=1, cbar_kws={'shrink': .8})\n",
        "plt.title(\"Correlation Heatmap (numeric columns only)\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9EE9zZYrHNeB",
      "metadata": {
        "id": "9EE9zZYrHNeB"
      },
      "outputs": [],
      "source": [
        "# Average prevailing wage by region (ascending)\n",
        "grouped_roe_pw = (\n",
        "    copydata.groupby('region_of_employment')['prevailing_wage']\n",
        "            .mean()\n",
        "            .sort_values()\n",
        ")\n",
        "print(grouped_roe_pw)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8vF3HwvGQV2h",
      "metadata": {
        "id": "8vF3HwvGQV2h"
      },
      "outputs": [],
      "source": [
        "# Mean wage per case_status (simple groupby)\n",
        "print(copydata.groupby('case_status')['prevailing_wage'].mean())\n",
        "\n",
        "# Convert to numeric (coerce errors → NaN)\n",
        "copydata['pw'] = pd.to_numeric(copydata['prevailing_wage'], errors='coerce')\n",
        "\n",
        "# Drop rows missing pw or case_status\n",
        "coydata = copydata.dropna(subset=['pw', 'case_status'])\n",
        "\n",
        "# IQR filter: keep rows within 1.5×IQR per group\n",
        "def iqr_filter(g):\n",
        "    q1, q3 = g['pw'].quantile([0.25, 0.75])\n",
        "    iqr = q3 - q1\n",
        "    low, high = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
        "    return g[(g['pw'] >= low) & (g['pw'] <= high)]\n",
        "\n",
        "# Apply per case_status, then recompute means\n",
        "copydata_iqr = copydata.groupby('case_status', group_keys=False).apply(iqr_filter)\n",
        "means_iqr = copydata_iqr.groupby('case_status')['pw'].mean()\n",
        "print(means_iqr.round(2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dressed-excuse",
      "metadata": {
        "id": "dressed-excuse"
      },
      "source": [
        "#### Does higher education increase the chances of visa certification for well-paid jobs abroad?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LRNklqXgHxGw",
      "metadata": {
        "id": "LRNklqXgHxGw"
      },
      "source": [
        "* Having higher education increases the likelihood of receiving a work visa in a linear fashion moving from highschool education to doctorate level.\n",
        "* About 37% of those that applied with a highschool education received work visas\n",
        "* A little over 60% of those that applied, with a bachelor's degree, received a work visa\n",
        "* Right at 80% of those that applied for a work visa with a master's degree received a work visa.\n",
        "* Right at 90% of those that applied for a work visa with a doctorate were certified.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "attended-current",
      "metadata": {
        "id": "attended-current"
      },
      "source": [
        "#### How does visa status vary across different continents?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xLCUI33wEBl4",
      "metadata": {
        "id": "xLCUI33wEBl4"
      },
      "source": [
        "* Visa status progresses in a linear fashion across contnenents in the following order. South America, North America, Ocenaia, Asia, Africa, and Europe.\n",
        "* South America has just below a 60% certification rate.\n",
        "* North America has just above a 60% approval rate.\n",
        "* Oceania has about a 63% certification rate\n",
        "* Asia has about a 65% certification rate.\n",
        "* Africa is around a 70% certification rate.\n",
        "* Europe has around an 80% certification rate.\n",
        "* Different continents see lower and higher rates of certification, by virtue of the continent alone."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "macro-decrease",
      "metadata": {
        "id": "macro-decrease"
      },
      "source": [
        "#### Does having prior work experience influence the chances of visa certification for career opportunities abroad?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DlN9q088GCX2",
      "metadata": {
        "id": "DlN9q088GCX2"
      },
      "source": [
        "* Those with job experience have just under an 80% chance of certification, while those without job experience have just under a 60% chance of certification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "changing-kansas",
      "metadata": {
        "id": "changing-kansas"
      },
      "source": [
        "#### Is the prevailing wage consistent across all regions of the US?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RO0isNLUG_3G",
      "metadata": {
        "id": "RO0isNLUG_3G"
      },
      "source": [
        "* The prevailing wage is fairly consistent between the south, northeast, and west regions as far as an average is concerned. The prevailing wage average differs by around 6000 dollars.\n",
        "* Island and Midwest regions are also very similar, as far as an average is concerned. There average variance is only 200 dollars.\n",
        "* The average prevailing wage between all regions has some significant variance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lesser-bacteria",
      "metadata": {
        "id": "lesser-bacteria"
      },
      "source": [
        "#### Does visa status vary with changes in the prevailing wage set to protect both local talent and foreign workers?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EzrsInE0Rv6l",
      "metadata": {
        "id": "EzrsInE0Rv6l"
      },
      "source": [
        "* The prevailing wage of those certified is an average of 10,000 dollars difference than those who are not certified. This indicates that the prevailing wage is protectiing both the foerign and US workers.\n",
        "* After dropping all outliers similar trends in the prevailing wage result. The difference is about 7000 dollars.\n",
        "* Either way those certified receieve 7-10 thousand dollars more in the prevailing wage than those who are not certified. This indicates a correct relationship with the prevailing wage and certification.\n",
        "* The prevailing wage does differ between the certified and the not certified. It does this in a correct way where those certified have a higher prevailing wage than those not certified."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "suspected-asthma",
      "metadata": {
        "id": "suspected-asthma"
      },
      "source": [
        "#### Does the unit of prevailing wage (Hourly, Weekly, etc.) have any impact on the likelihood of visa application certification?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-6JscDuTWU94",
      "metadata": {
        "id": "-6JscDuTWU94"
      },
      "source": [
        "* Houlry wage receives about a 37% approval rate.\n",
        "* Weekly and monthly receive around a 60% approval rate.\n",
        "* Yearly receives around a 70% approval rate.\n",
        "* In some way it seems that the unit does have correlation with certification verse non-certification."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qBWlk20UBUAx",
      "metadata": {
        "id": "qBWlk20UBUAx"
      },
      "source": [
        "## Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "allied-association",
      "metadata": {
        "id": "allied-association"
      },
      "source": [
        "### Outlier Check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "H76eABeDCJWY",
      "metadata": {
        "id": "H76eABeDCJWY"
      },
      "outputs": [],
      "source": [
        "def mark_outliers_iqr(g, col='pw', k=1.5, add_bounds=False):\n",
        "    \"\"\"\n",
        "    Add a boolean flag for IQR outliers within a group.\n",
        "\n",
        "    g : DataFrame (one group from groupby)\n",
        "    col : column name to check (e.g., 'pw')\n",
        "    k : fence multiplier (1.5 = standard; 3.0 = 'extreme' outliers)\n",
        "    add_bounds : if True, also add q1/q3/low/high columns for reference\n",
        "    \"\"\"\n",
        "    s = pd.to_numeric(g[col], errors='coerce')\n",
        "    q1 = s.quantile(0.25)\n",
        "    q3 = s.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "\n",
        "    # Handle degenerate groups (iqr == 0 or all NaN)\n",
        "    if pd.isna(iqr) or iqr == 0:\n",
        "        out = g.copy()\n",
        "        out[f'{col}_outlier_iqr'] = False\n",
        "        if add_bounds:\n",
        "            out[f'{col}_q1'], out[f'{col}_q3']   = q1, q3\n",
        "            out[f'{col}_low'], out[f'{col}_high'] = q1, q3\n",
        "        return out\n",
        "\n",
        "    low, high = q1 - k*iqr, q3 + k*iqr\n",
        "    out = g.copy()\n",
        "    out[f'{col}_outlier_iqr'] = (s < low) | (s > high)\n",
        "    if add_bounds:\n",
        "        out[f'{col}_q1'], out[f'{col}_q3']   = q1, q3\n",
        "        out[f'{col}_low'], out[f'{col}_high'] = low, high\n",
        "    return out\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YQGFkmeshvWz",
      "metadata": {
        "id": "YQGFkmeshvWz"
      },
      "outputs": [],
      "source": [
        "# Work on a copy of the dataset to avoid altering the original\n",
        "df = copydata.copy()\n",
        "\n",
        "# Convert 'prevailing_wage' to numeric\n",
        "# - Non-numeric entries become NaN (via errors='coerce')\n",
        "# - Stores in a new column 'pw'\n",
        "df['pw'] = pd.to_numeric(df['prevailing_wage'], errors='coerce')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "QeVBzY6xhy_m",
      "metadata": {
        "id": "QeVBzY6xhy_m"
      },
      "outputs": [],
      "source": [
        "# Flag outliers in 'pw' using IQR (k=1.5, keep bounds too)\n",
        "df_marked = mark_outliers_iqr(df, col='pw', k=1.5, add_bounds=True)\n",
        "\n",
        "# Separate into outliers and inliers\n",
        "outliers = df_marked[df_marked['pw_outlier_iqr']]\n",
        "inliers  = df_marked[~df_marked['pw_outlier_iqr']]\n",
        "\n",
        "# Shapes (rows, cols) of each set\n",
        "print(outliers.shape)\n",
        "print(inliers.shape)\n",
        "\n",
        "# Total rows across both sets\n",
        "total_rows = outliers.shape[0] + inliers.shape[0]\n",
        "\n",
        "# Outlier and inlier counts + percentages\n",
        "print(f\"outliers rows: {outliers.shape[0]:,} / {total_rows:,} ({outliers.shape[0]/total_rows:.2%})\")\n",
        "print(f\"inliers  rows: {inliers.shape[0]:,} / {total_rows:,} ({inliers.shape[0]/total_rows:.2%})\")\n",
        "\n",
        "# Notes:\n",
        "# - df_marked keeps original data + flag column ('pw_outlier_iqr') and IQR bounds.\n",
        "# - Shapes confirm split sizes; percentages check outlier prevalence.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "flexible-independence",
      "metadata": {
        "id": "flexible-independence"
      },
      "source": [
        "### Data Preparation for modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YsRIntveCJ9_",
      "metadata": {
        "id": "YsRIntveCJ9_"
      },
      "outputs": [],
      "source": [
        "# 0) Split target and features\n",
        "X = data.drop(columns=['case_status'])\n",
        "y = data['case_status']\n",
        "\n",
        "# 1) Make binary target (optional: set minority as positive=1)\n",
        "minority = y.value_counts().idxmin()\n",
        "y_bin = (y == minority).astype(int)\n",
        "\n",
        "# 2) Train / Test split (e.g., 80/20)\n",
        "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
        "    X, y_bin, test_size=0.20, random_state=1, stratify=y_bin\n",
        ")\n",
        "\n",
        "# 3) From remaining 80%, carve out Validation (e.g., 20% of original → 25% of remaining)\n",
        "X_train, X_val, y_train, y_val = train_test_split(\n",
        "    X_train_full, y_train_full, test_size=0.25, random_state=1, stratify=y_train_full\n",
        ")\n",
        "\n",
        "# 4) Identify categorical columns from TRAIN ONLY\n",
        "cat_cols = X_train.select_dtypes(include=['object', 'category', 'bool']).columns.tolist()\n",
        "\n",
        "# If you have numeric-coded categoricals, declare them and cast:\n",
        "# num_as_cats = ['colA', 'colB']\n",
        "# for df in (X_train, X_val, X_test):\n",
        "#     df[num_as_cats] = df[num_as_cats].astype('category')\n",
        "# cat_cols = sorted(set(cat_cols).union(num_as_cats))\n",
        "\n",
        "# 5) One-hot encode each split separately, then align to TRAIN columns\n",
        "X_train_d = pd.get_dummies(X_train, columns=cat_cols, drop_first=True, dtype='uint8')\n",
        "X_val_d   = pd.get_dummies(X_val,   columns=cat_cols, drop_first=True, dtype='uint8')\n",
        "X_test_d  = pd.get_dummies(X_test,  columns=cat_cols, drop_first=True, dtype='uint8')\n",
        "\n",
        "# Align val/test to train’s columns (drop unknowns, add missing as 0)\n",
        "X_val_d  = X_val_d.reindex(columns=X_train_d.columns, fill_value=0)\n",
        "X_test_d = X_test_d.reindex(columns=X_train_d.columns, fill_value=0)\n",
        "\n",
        "# 6) Quick sanity checks\n",
        "print(\"Train:\", X_train_d.shape, \"Val:\", X_val_d.shape, \"Test:\", X_test_d.shape)\n",
        "print(\"y train/val/test:\", y_train.shape, y_val.shape, y_test.shape)\n",
        "print(\"Class balance (train):\\n\", y_train.value_counts(normalize=True))\n",
        "print(\"Class balance (val):\\n\",   y_val.value_counts(normalize=True))\n",
        "print(\"Class balance (test):\\n\",  y_test.value_counts(normalize=True))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dr7q6-dbfiQB",
      "metadata": {
        "id": "dr7q6-dbfiQB"
      },
      "source": [
        "## Model Building"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rrlw9AVcqk37",
      "metadata": {
        "id": "rrlw9AVcqk37"
      },
      "source": [
        "### Model Evaluation Criterion"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vw4vUNvIql3X",
      "metadata": {
        "id": "vw4vUNvIql3X"
      },
      "source": [
        "- Choose the primary metric to evaluate the model on\n",
        "- Elaborate on the rationale behind choosing the metric\n",
        "- Xgboost is going to likely be the ideal model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "PepZef6UqpNa",
      "metadata": {
        "id": "PepZef6UqpNa"
      },
      "source": [
        "First, let's create functions to calculate different metrics and confusion matrix so that we don't have to use the same code repeatedly for each model.\n",
        "* The `model_performance_classification_sklearn` function will be used to check the model performance of models.\n",
        "* The `confusion_matrix_sklearn` function will be used to plot the confusion matrix."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mexican-database",
      "metadata": {
        "id": "mexican-database"
      },
      "outputs": [],
      "source": [
        "# defining a function to compute different metrics to check performance of a classification model built using sklearn\n",
        "\n",
        "\n",
        "def model_performance_classification_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    Function to compute different metrics to check classification model performance\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "\n",
        "    # predicting using the independent variables\n",
        "    pred = model.predict(predictors)\n",
        "\n",
        "    acc = accuracy_score(target, pred)  # to compute Accuracy\n",
        "    recall = recall_score(target, pred)  # to compute Recall\n",
        "    precision = precision_score(target, pred)  # to compute Precision\n",
        "    f1 = f1_score(target, pred)  # to compute F1-score\n",
        "\n",
        "    # creating a dataframe of metrics\n",
        "    df_perf = pd.DataFrame(\n",
        "        {\"Accuracy\": acc, \"Recall\": recall, \"Precision\": precision, \"F1\": f1,},\n",
        "        index=[0],\n",
        "    )\n",
        "\n",
        "    return df_perf"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "recreational-topic",
      "metadata": {
        "id": "recreational-topic"
      },
      "outputs": [],
      "source": [
        "def confusion_matrix_sklearn(model, predictors, target):\n",
        "    \"\"\"\n",
        "    To plot the confusion_matrix with percentages\n",
        "\n",
        "    model: classifier\n",
        "    predictors: independent variables\n",
        "    target: dependent variable\n",
        "    \"\"\"\n",
        "    y_pred = model.predict(predictors)\n",
        "    cm = confusion_matrix(target, y_pred)\n",
        "    labels = np.asarray(\n",
        "        [\n",
        "            [\"{0:0.0f}\".format(item) + \"\\n{0:.2%}\".format(item / cm.flatten().sum())]\n",
        "            for item in cm.flatten()\n",
        "        ]\n",
        "    ).reshape(2, 2)\n",
        "\n",
        "    plt.figure(figsize=(6, 4))\n",
        "    sns.heatmap(cm, annot=labels, fmt=\"\")\n",
        "    plt.ylabel(\"True label\")\n",
        "    plt.xlabel(\"Predicted label\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0QZZoxoDcoDm",
      "metadata": {
        "id": "0QZZoxoDcoDm"
      },
      "source": [
        "#### Defining scorer to be used for cross-validation and hyperparameter tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C8eioikMTipz",
      "metadata": {
        "id": "C8eioikMTipz"
      },
      "source": [
        "* Both false positives and false negatives need to be avoided. For this reason the f1 score is the most ideal metric for this situation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XWoHuUpjbp0_",
      "metadata": {
        "id": "XWoHuUpjbp0_"
      },
      "source": [
        "**We are now done with pre-processing and evaluation criterion, so let's start building the model.**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4fI98GOV0pTY",
      "metadata": {
        "id": "4fI98GOV0pTY"
      },
      "source": [
        "### Model building with Original data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vOPuYn7dCGjx",
      "metadata": {
        "id": "vOPuYn7dCGjx"
      },
      "outputs": [],
      "source": [
        "# 1) Train Bagging with defaults (no hyperparameter tuning)\n",
        "bag = BaggingClassifier(random_state=1)   # defaults: base estimator = DecisionTreeClassifier(), n_estimators=10\n",
        "bag.fit(X_train_d, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r7QkF0KDzSR6",
      "metadata": {
        "id": "r7QkF0KDzSR6"
      },
      "outputs": [],
      "source": [
        "# Train-set performance\n",
        "bagging_estimator_score_train = model_performance_classification_sklearn(\n",
        "    bag, X_train_d, y_train\n",
        ")\n",
        "bagging_estimator_score_train\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EaeZjLt3z5H5",
      "metadata": {
        "id": "EaeZjLt3z5H5"
      },
      "outputs": [],
      "source": [
        "# Validation-set performance\n",
        "bagging_estimator_score_val = model_performance_classification_sklearn(\n",
        "    bag, X_val_d, y_val\n",
        ")\n",
        "bagging_estimator_score_val\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qvjPdSWo0usC",
      "metadata": {
        "id": "qvjPdSWo0usC"
      },
      "outputs": [],
      "source": [
        "bagging_estimator_cm = confusion_matrix_sklearn(bag, X_val_d, y_val)\n",
        "bagging_estimator_cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LMCnWwu_1-Oz",
      "metadata": {
        "id": "LMCnWwu_1-Oz"
      },
      "outputs": [],
      "source": [
        "dt = DecisionTreeClassifier(random_state=1)\n",
        "dt.fit(X_train_d, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SeHqzbzh2flq",
      "metadata": {
        "id": "SeHqzbzh2flq"
      },
      "outputs": [],
      "source": [
        "dt_estimator_score_train = model_performance_classification_sklearn(\n",
        "    dt, X_train_d, y_train\n",
        ")\n",
        "dt_estimator_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mhp098Yu2kS0",
      "metadata": {
        "id": "mhp098Yu2kS0"
      },
      "outputs": [],
      "source": [
        "dt_estimator_score_val = model_performance_classification_sklearn(\n",
        "    dt, X_val_d, y_val\n",
        ")\n",
        "dt_estimator_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_j7ANgp22vca",
      "metadata": {
        "id": "_j7ANgp22vca"
      },
      "outputs": [],
      "source": [
        "dt_estimator_cm = confusion_matrix_sklearn(dt, X_val_d, y_val)\n",
        "dt_estimator_cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6G6K2lKO5D8n",
      "metadata": {
        "id": "6G6K2lKO5D8n"
      },
      "outputs": [],
      "source": [
        "rf = RandomForestClassifier(random_state=1)\n",
        "rf.fit(X_train_d, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ajos1Xck5rVS",
      "metadata": {
        "id": "Ajos1Xck5rVS"
      },
      "outputs": [],
      "source": [
        "rf_estimator_score_train = model_performance_classification_sklearn(\n",
        "    rf, X_train_d, y_train\n",
        ")\n",
        "rf_estimator_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2KFdU6t96ST-",
      "metadata": {
        "id": "2KFdU6t96ST-"
      },
      "outputs": [],
      "source": [
        "rf_estimator_score_val = model_performance_classification_sklearn(\n",
        "    rf, X_val_d, y_val\n",
        ")\n",
        "rf_estimator_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "rs1NIsp_6Wkg",
      "metadata": {
        "id": "rs1NIsp_6Wkg"
      },
      "outputs": [],
      "source": [
        "rf_estimator_cm = confusion_matrix_sklearn(rf, X_val_d, y_val)\n",
        "rf_estimator_cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wm4-BUDV6fok",
      "metadata": {
        "id": "wm4-BUDV6fok"
      },
      "outputs": [],
      "source": [
        "abc = AdaBoostClassifier(random_state=1)\n",
        "abc.fit(X_train_d, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yBulGknO7fHL",
      "metadata": {
        "id": "yBulGknO7fHL"
      },
      "outputs": [],
      "source": [
        "abc_estimator_score_train = model_performance_classification_sklearn(\n",
        "    abc, X_train_d, y_train\n",
        ")\n",
        "abc_estimator_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dqownFge7lkU",
      "metadata": {
        "id": "dqownFge7lkU"
      },
      "outputs": [],
      "source": [
        "abc_estimator_score_val = model_performance_classification_sklearn(\n",
        "    abc, X_val_d, y_val\n",
        ")\n",
        "abc_estimator_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hU0KPRSf7ojP",
      "metadata": {
        "id": "hU0KPRSf7ojP"
      },
      "outputs": [],
      "source": [
        "abc_estimator_cm = confusion_matrix_sklearn(abc, X_val_d, y_val)\n",
        "abc_estimator_cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ybuXm_id8Cw7",
      "metadata": {
        "id": "ybuXm_id8Cw7"
      },
      "outputs": [],
      "source": [
        "xgb= XGBClassifier(random_state=1)\n",
        "xgb.fit(X_train_d,y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mJ9ip5MQ9qoC",
      "metadata": {
        "id": "mJ9ip5MQ9qoC"
      },
      "outputs": [],
      "source": [
        "xgb_estimator_score_train= model_performance_classification_sklearn(xgb,X_train_d, y_train,)\n",
        "xgb_estimator_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xXl6hNbf9u3j",
      "metadata": {
        "id": "xXl6hNbf9u3j"
      },
      "outputs": [],
      "source": [
        "xgb_estimator_score_val= model_performance_classification_sklearn(xgb,X_val_d, y_val,)\n",
        "xgb_estimator_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aNjo_M5K9w1B",
      "metadata": {
        "id": "aNjo_M5K9w1B"
      },
      "outputs": [],
      "source": [
        "xgb_estimator_cm = confusion_matrix_sklearn(xgb, X_val_d, y_val)\n",
        "xgb_estimator_cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UQg3xg7f9-15",
      "metadata": {
        "id": "UQg3xg7f9-15"
      },
      "outputs": [],
      "source": [
        "gb = GradientBoostingClassifier(random_state=1)\n",
        "gb.fit(X_train_d, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tm0HsLvCAK7P",
      "metadata": {
        "id": "tm0HsLvCAK7P"
      },
      "outputs": [],
      "source": [
        "gb_estimator_score_train = model_performance_classification_sklearn(\n",
        "    gb, X_train_d, y_train\n",
        ")\n",
        "gb_estimator_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "t-hXL8Q-ANuM",
      "metadata": {
        "id": "t-hXL8Q-ANuM"
      },
      "outputs": [],
      "source": [
        "gb_estimator_score_val = model_performance_classification_sklearn(\n",
        "    gb, X_val_d, y_val\n",
        ")\n",
        "gb_estimator_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "S-R3jXFHAQQ1",
      "metadata": {
        "id": "S-R3jXFHAQQ1"
      },
      "outputs": [],
      "source": [
        "gb_estimator_cm = confusion_matrix_sklearn(gb, X_val_d, y_val)\n",
        "gb_estimator_cm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RmhhGvFfAaeZ",
      "metadata": {
        "id": "RmhhGvFfAaeZ"
      },
      "source": [
        "* Gradient boosting and adaboost both generalized well. But they were both underfit with poor f1 scores, both under 60.\n",
        "* Every other model was overfit and also performed poorly with the f1 scores on validation sets."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "C91_6Swtbp1A",
      "metadata": {
        "id": "C91_6Swtbp1A"
      },
      "source": [
        "### Model Building with Oversampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PzWwDZrwCHJJ",
      "metadata": {
        "id": "PzWwDZrwCHJJ"
      },
      "outputs": [],
      "source": [
        "# Ensure y is 1-D\n",
        "y_train_1d = np.asarray(y_train).ravel()\n",
        "\n",
        "def counts(y):\n",
        "    return pd.Series(y).value_counts(dropna=False)\n",
        "\n",
        "print(\"Before OverSampling (value_counts):\")\n",
        "print(counts(y_train_1d), \"\\n\")\n",
        "\n",
        "# OPTIONAL: choose k_neighbors safely if the minority class is tiny\n",
        "minority_count = counts(y_train_1d).min()\n",
        "k = max(1, min(5, minority_count - 1))  # keep k < minority_count\n",
        "\n",
        "sm = SMOTE(sampling_strategy=1.0, k_neighbors=k, random_state=1)\n",
        "\n",
        "# IMPORTANT: use the one-hot encoded training matrix\n",
        "X_train_over, y_train_over = sm.fit_resample(X_train_d, y_train_1d)\n",
        "\n",
        "print(\"After OverSampling (value_counts):\")\n",
        "print(counts(y_train_over), \"\\n\")\n",
        "\n",
        "print(\"After OverSampling, the shape of train_X:\", X_train_over.shape)\n",
        "print(\"After OverSampling, the shape of train_y:\", y_train_over.shape, \"\\n\")\n",
        "\n",
        "# If you specifically want counts for label 0 and 1 (only if labels truly are 0/1):\n",
        "def lbl_count(y, lbl): return int(np.sum(np.asarray(y).ravel() == lbl))\n",
        "print(\"Before OverSampling, count of label '1':\", lbl_count(y_train_1d, 1))\n",
        "print(\"Before OverSampling, count of label '0':\", lbl_count(y_train_1d, 0))\n",
        "print(\"After OverSampling, count of label '1':\", lbl_count(y_train_over, 1))\n",
        "print(\"After OverSampling, count of label '0':\", lbl_count(y_train_over, 0))\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "M7xJ9nj3vmXC",
      "metadata": {
        "id": "M7xJ9nj3vmXC"
      },
      "outputs": [],
      "source": [
        "bag_over = BaggingClassifier(random_state=1)\n",
        "bag_over.fit(X_train_over, y_train_over)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UMvfCLntwsmK",
      "metadata": {
        "id": "UMvfCLntwsmK"
      },
      "outputs": [],
      "source": [
        "bagging_estimator_score_train_over= model_performance_classification_sklearn(bag_over,X_train_over, y_train_over,)\n",
        "bagging_estimator_score_train_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bLtPrdtrxeAJ",
      "metadata": {
        "id": "bLtPrdtrxeAJ"
      },
      "outputs": [],
      "source": [
        "bagging_estimator_score_val_over= model_performance_classification_sklearn(bag_over,X_val_d, y_val,)\n",
        "bagging_estimator_score_val_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gByMX0ItyLVZ",
      "metadata": {
        "id": "gByMX0ItyLVZ"
      },
      "outputs": [],
      "source": [
        "bagging_estimator_cm_over = confusion_matrix_sklearn(bag_over, X_val_d, y_val)\n",
        "bagging_estimator_cm_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "anvkVyQTyhAp",
      "metadata": {
        "id": "anvkVyQTyhAp"
      },
      "outputs": [],
      "source": [
        "dt_over = DecisionTreeClassifier(random_state=1)\n",
        "dt_over.fit(X_train_over, y_train_over)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qTrkQdUmy8RR",
      "metadata": {
        "id": "qTrkQdUmy8RR"
      },
      "outputs": [],
      "source": [
        "dt_estimator_score_train_over= model_performance_classification_sklearn(dt_over,X_train_over, y_train_over,)\n",
        "dt_estimator_score_train_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6_SfJy-zRCo",
      "metadata": {
        "id": "f6_SfJy-zRCo"
      },
      "outputs": [],
      "source": [
        "dt_estimator_score_val_over= model_performance_classification_sklearn(dt_over,X_val_d, y_val,)\n",
        "dt_estimator_score_val_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ochM4ICJzY8w",
      "metadata": {
        "id": "ochM4ICJzY8w"
      },
      "outputs": [],
      "source": [
        "dt_estimator_cm_over = confusion_matrix_sklearn(dt_over, X_val_d, y_val)\n",
        "dt_estimator_cm_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_Vf6cePuzfPK",
      "metadata": {
        "id": "_Vf6cePuzfPK"
      },
      "outputs": [],
      "source": [
        "rf_over = RandomForestClassifier(random_state=1)\n",
        "rf_over.fit(X_train_over, y_train_over)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3AmBiNgRz1rQ",
      "metadata": {
        "id": "3AmBiNgRz1rQ"
      },
      "outputs": [],
      "source": [
        "rf_estimator_score_train_over= model_performance_classification_sklearn(rf_over,X_train_over, y_train_over,)\n",
        "rf_estimator_score_train_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kSoO2CsI1Bq3",
      "metadata": {
        "id": "kSoO2CsI1Bq3"
      },
      "outputs": [],
      "source": [
        "rf_estimator_score_val_over= model_performance_classification_sklearn(rf_over,X_val_d, y_val,)\n",
        "rf_estimator_score_val_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Q8sB6NJF1YTh",
      "metadata": {
        "id": "Q8sB6NJF1YTh"
      },
      "outputs": [],
      "source": [
        "rf_estimator_cm_over = confusion_matrix_sklearn(rf_over, X_val_d, y_val)\n",
        "rf_estimator_cm_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZtIuAY_u1cQI",
      "metadata": {
        "id": "ZtIuAY_u1cQI"
      },
      "outputs": [],
      "source": [
        "abc_over = AdaBoostClassifier(random_state=1)\n",
        "abc_over.fit(X_train_over, y_train_over)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2VSo_kT82hVA",
      "metadata": {
        "id": "2VSo_kT82hVA"
      },
      "outputs": [],
      "source": [
        "abc_estimator_score_train_over= model_performance_classification_sklearn(abc_over,X_train_over, y_train_over,)\n",
        "abc_estimator_score_train_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cctQpcMk2k4H",
      "metadata": {
        "id": "cctQpcMk2k4H"
      },
      "outputs": [],
      "source": [
        "abc_estimator_score_val_over= model_performance_classification_sklearn(abc_over,X_val_d, y_val,)\n",
        "abc_estimator_score_val_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Cj9TlIY12zB4",
      "metadata": {
        "id": "Cj9TlIY12zB4"
      },
      "outputs": [],
      "source": [
        "abc_estimator_cm_over = confusion_matrix_sklearn(abc_over, X_val_d, y_val)\n",
        "abc_estimator_cm_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19-f0NSe3Eqs",
      "metadata": {
        "id": "19-f0NSe3Eqs"
      },
      "outputs": [],
      "source": [
        "xgb_estimator_over=XGBClassifier(random_state=1)\n",
        "xgb_estimator_over.fit(X_train_over,y_train_over)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DirGU4PF3vus",
      "metadata": {
        "id": "DirGU4PF3vus"
      },
      "outputs": [],
      "source": [
        "xgb_estimator_over_score_train= model_performance_classification_sklearn(xgb_estimator_over,X_train_over, y_train_over,)\n",
        "xgb_estimator_over_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4L8vWGTC3zVZ",
      "metadata": {
        "id": "4L8vWGTC3zVZ"
      },
      "outputs": [],
      "source": [
        "xgb_estimator_over_score_val= model_performance_classification_sklearn(xgb_estimator_over,X_val_d, y_val,)\n",
        "xgb_estimator_over_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J3SBoW-T36at",
      "metadata": {
        "id": "J3SBoW-T36at"
      },
      "outputs": [],
      "source": [
        "xgb_estimator_over_cm = confusion_matrix_sklearn(xgb_estimator_over, X_val_d, y_val)\n",
        "xgb_estimator_over_cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CutOG7Fz4DTr",
      "metadata": {
        "id": "CutOG7Fz4DTr"
      },
      "outputs": [],
      "source": [
        "gb_over = GradientBoostingClassifier(random_state=1)\n",
        "gb_over.fit(X_train_over, y_train_over)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lS8BPf-B49Bo",
      "metadata": {
        "id": "lS8BPf-B49Bo"
      },
      "outputs": [],
      "source": [
        "gb_estimator_score_train_over= model_performance_classification_sklearn(gb_over,X_train_over, y_train_over,)\n",
        "gb_estimator_score_train_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "pv_hA24S6R-w",
      "metadata": {
        "id": "pv_hA24S6R-w"
      },
      "outputs": [],
      "source": [
        "gb_estimator_score_val_over= model_performance_classification_sklearn(gb_over,X_val_d, y_val,)\n",
        "gb_estimator_score_val_over"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4vNlc7Xs6Wno",
      "metadata": {
        "id": "4vNlc7Xs6Wno"
      },
      "outputs": [],
      "source": [
        "gb_estimator_cm_over = confusion_matrix_sklearn(gb_over, X_val_d, y_val)\n",
        "gb_estimator_cm_over"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "uSgJJU7uItug",
      "metadata": {
        "id": "uSgJJU7uItug"
      },
      "source": [
        "* All oversampled data models were overfit and did not generalize well"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fYLfDmHvbp1B",
      "metadata": {
        "id": "fYLfDmHvbp1B"
      },
      "source": [
        "### Model Building with Undersampled data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LAvFgREaCHqm",
      "metadata": {
        "id": "LAvFgREaCHqm"
      },
      "outputs": [],
      "source": [
        "# Make sure y is 1-D\n",
        "y_train_1d = np.asarray(y_train).ravel()\n",
        "\n",
        "print(\"Before Under Sampling (value_counts):\")\n",
        "print(pd.Series(y_train_1d).value_counts(), \"\\n\")\n",
        "\n",
        "# Perform random undersampling on the ONE-HOT TRAIN set\n",
        "rus = RandomUnderSampler(random_state=1, sampling_strategy=1.0)  # 1.0 => balance classes 1:1\n",
        "X_train_un, y_train_un = rus.fit_resample(X_train_d, y_train_1d)  # <-- use X_train_d\n",
        "\n",
        "print(\"After Under Sampling (value_counts):\")\n",
        "print(pd.Series(y_train_un).value_counts(), \"\\n\")\n",
        "\n",
        "print(\"After Under Sampling, the shape of train_X:\", X_train_un.shape)\n",
        "print(\"After Under Sampling, the shape of train_y:\", y_train_un.shape, \"\\n\")\n",
        "\n",
        "# OPTIONAL: if your labels are actually 0/1 and you want explicit counts\n",
        "if set(np.unique(y_train_1d)) == {0, 1}:\n",
        "    print(\"Before Under Sampling, count of label '1':\", np.sum(y_train_1d == 1))\n",
        "    print(\"Before Under Sampling, count of label '0':\", np.sum(y_train_1d == 0), \"\\n\")\n",
        "    print(\"After Under Sampling, count of label '1':\", np.sum(y_train_un == 1))\n",
        "    print(\"After Under Sampling, count of label '0':\", np.sum(y_train_un == 0), \"\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tzZQs8mT6nvL",
      "metadata": {
        "id": "tzZQs8mT6nvL"
      },
      "outputs": [],
      "source": [
        "bag_under = BaggingClassifier(random_state=1)\n",
        "bag_under.fit(X_train_un, y_train_un)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "TWjuHGnr67Gw",
      "metadata": {
        "id": "TWjuHGnr67Gw"
      },
      "outputs": [],
      "source": [
        "bagging_estimator_under_score_train= model_performance_classification_sklearn(bag_under,X_train_un, y_train_un,)\n",
        "bagging_estimator_under_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "afmQxvdN7D6A",
      "metadata": {
        "id": "afmQxvdN7D6A"
      },
      "outputs": [],
      "source": [
        "bagging_estimator_under_score_val= model_performance_classification_sklearn(bag_under,X_val_d, y_val,)\n",
        "bagging_estimator_under_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "AynM7if67He_",
      "metadata": {
        "id": "AynM7if67He_"
      },
      "outputs": [],
      "source": [
        "bagging_estimator_under_cm = confusion_matrix_sklearn(bag_under, X_val_d, y_val)\n",
        "bagging_estimator_under_cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "sgcyVtNw77SY",
      "metadata": {
        "id": "sgcyVtNw77SY"
      },
      "outputs": [],
      "source": [
        "dt_under = DecisionTreeClassifier(random_state=1)\n",
        "dt_under.fit(X_train_un, y_train_un)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbqw79Ym8Wlh",
      "metadata": {
        "id": "cbqw79Ym8Wlh"
      },
      "outputs": [],
      "source": [
        "dt_under_score_train= model_performance_classification_sklearn(dt_under,X_train_un, y_train_un,)\n",
        "dt_under_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cV-sC5yZ8bFH",
      "metadata": {
        "id": "cV-sC5yZ8bFH"
      },
      "outputs": [],
      "source": [
        "dt_under_score_val= model_performance_classification_sklearn(dt_under,X_val_d, y_val,)\n",
        "dt_under_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vdu8-XnV8fxQ",
      "metadata": {
        "id": "vdu8-XnV8fxQ"
      },
      "outputs": [],
      "source": [
        "dt_under_cm = confusion_matrix_sklearn(dt_under, X_val_d, y_val)\n",
        "dt_under_cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "u_mbp9y18jWT",
      "metadata": {
        "id": "u_mbp9y18jWT"
      },
      "outputs": [],
      "source": [
        "rf_under = RandomForestClassifier(random_state=1)\n",
        "rf_under.fit(X_train_un, y_train_un)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CO2k3DPo82Lq",
      "metadata": {
        "id": "CO2k3DPo82Lq"
      },
      "outputs": [],
      "source": [
        "rf_under_score_train= model_performance_classification_sklearn(rf_under,X_train_un, y_train_un,)\n",
        "rf_under_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "mPOl8Wzm9a2Y",
      "metadata": {
        "id": "mPOl8Wzm9a2Y"
      },
      "outputs": [],
      "source": [
        "rf_under_score_val= model_performance_classification_sklearn(rf_under,X_val_d, y_val,)\n",
        "rf_under_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BniTjUL-9sow",
      "metadata": {
        "id": "BniTjUL-9sow"
      },
      "outputs": [],
      "source": [
        "rf_under_cm = confusion_matrix_sklearn(rf_under, X_val_d, y_val)\n",
        "rf_under_cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HXU_3cW8956N",
      "metadata": {
        "id": "HXU_3cW8956N"
      },
      "outputs": [],
      "source": [
        "abc_under = AdaBoostClassifier(random_state=1)\n",
        "abc_under.fit(X_train_un, y_train_un)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "A55mjkng9vV7",
      "metadata": {
        "id": "A55mjkng9vV7"
      },
      "outputs": [],
      "source": [
        "abc_under_score_train= model_performance_classification_sklearn(abc_under,X_train_un, y_train_un,)\n",
        "abc_under_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1djwxYjG-D6L",
      "metadata": {
        "id": "1djwxYjG-D6L"
      },
      "outputs": [],
      "source": [
        "abc_under_score_val= model_performance_classification_sklearn(abc_under,X_val_d, y_val,)\n",
        "abc_under_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yFtUCjMF-Fmw",
      "metadata": {
        "id": "yFtUCjMF-Fmw"
      },
      "outputs": [],
      "source": [
        "abc_under_cm = confusion_matrix_sklearn(abc_under, X_val_d, y_val)\n",
        "abc_under_cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "JLQZJMdq-k04",
      "metadata": {
        "id": "JLQZJMdq-k04"
      },
      "outputs": [],
      "source": [
        "xgb_estimator_under=XGBClassifier(random_state=1)\n",
        "xgb_estimator_under.fit(X_train_un,y_train_un)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qHrgtl8t-n9r",
      "metadata": {
        "id": "qHrgtl8t-n9r"
      },
      "outputs": [],
      "source": [
        "xgb_estimator_under_score_train= model_performance_classification_sklearn(xgb_estimator_under,X_train_un, y_train_un,)\n",
        "xgb_estimator_under_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HW4Lloxe-poS",
      "metadata": {
        "id": "HW4Lloxe-poS"
      },
      "outputs": [],
      "source": [
        "xgb_estimator_under_score_val= model_performance_classification_sklearn(xgb_estimator_under,X_val_d, y_val,)\n",
        "xgb_estimator_under_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ly5hOmfR-rMi",
      "metadata": {
        "id": "Ly5hOmfR-rMi"
      },
      "outputs": [],
      "source": [
        "xgb_estimator_under_cm = confusion_matrix_sklearn(xgb_estimator_under, X_val_d, y_val)\n",
        "xgb_estimator_under_cm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "O0UN3Nam-33z",
      "metadata": {
        "id": "O0UN3Nam-33z"
      },
      "outputs": [],
      "source": [
        "gb_under = GradientBoostingClassifier(random_state=1)\n",
        "gb_under.fit(X_train_un, y_train_un)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XB_d8fA3-8Qo",
      "metadata": {
        "id": "XB_d8fA3-8Qo"
      },
      "outputs": [],
      "source": [
        "gb_estimator_under_score_train= model_performance_classification_sklearn(gb_under,X_train_un, y_train_un,)\n",
        "gb_estimator_under_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kbQCvh36_BQK",
      "metadata": {
        "id": "kbQCvh36_BQK"
      },
      "outputs": [],
      "source": [
        "gb_estimator_under_score_val= model_performance_classification_sklearn(gb_under,X_val_d, y_val,)\n",
        "gb_estimator_under_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lKmiKT8X_DTf",
      "metadata": {
        "id": "lKmiKT8X_DTf"
      },
      "outputs": [],
      "source": [
        "gb_estimator_under_cm = confusion_matrix_sklearn(gb_under, X_val_d, y_val)\n",
        "gb_estimator_under_cm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YRXlXheEJA8z",
      "metadata": {
        "id": "YRXlXheEJA8z"
      },
      "source": [
        "* The abc undersampled model had good generalizability with the f1 score. However, it still performed poorly. The rest were overfit"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Cg_OREBD1NOy",
      "metadata": {
        "id": "Cg_OREBD1NOy"
      },
      "source": [
        "## Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "nVuzJqcU8cd0",
      "metadata": {
        "id": "nVuzJqcU8cd0"
      },
      "source": [
        "**Best practices for hyperparameter tuning in AdaBoost:**\n",
        "\n",
        "`n_estimators`:\n",
        "\n",
        "- Start with a specific number (50 is used in general) and increase in steps: 50, 75, 85, 100\n",
        "\n",
        "- Use fewer estimators (e.g., 50 to 100) if using complex base learners (like deeper decision trees)\n",
        "\n",
        "- Use more estimators (e.g., 100 to 150) when learning rate is low (e.g., 0.1 or lower)\n",
        "\n",
        "- Avoid very high values unless performance keeps improving on validation\n",
        "\n",
        "`learning_rate`:\n",
        "\n",
        "- Common values to try: 1.0, 0.5, 0.1, 0.01\n",
        "\n",
        "- Use 1.0 for faster training, suitable for fewer estimators\n",
        "\n",
        "- Use 0.1 or 0.01 when using more estimators to improve generalization\n",
        "\n",
        "- Avoid very small values (< 0.01) unless you plan to use many estimators (e.g., >500) and have sufficient data\n",
        "\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ovvcf1LDF5yf",
      "metadata": {
        "id": "Ovvcf1LDF5yf"
      },
      "outputs": [],
      "source": [
        "# ----- labels & scorer (minority = positive) -----\n",
        "y_train_1d = np.asarray(y_train).ravel()\n",
        "pos_label = pd.Series(y_train_1d).value_counts().idxmin()\n",
        "f1_scorer = make_scorer(f1_score, pos_label=pos_label, zero_division=0)\n",
        "\n",
        "# ----- AdaBoost + base tree (depth fixed to 4 per your spec) -----\n",
        "base_dt = DecisionTreeClassifier(max_depth=4, random_state=1)\n",
        "\n",
        "# Handle old/new sklearn API\n",
        "use_new = \"estimator\" in AdaBoostClassifier().get_params()\n",
        "if use_new:\n",
        "    abc_tuned = AdaBoostClassifier(estimator=base_dt, random_state=1)\n",
        "    param_grid = {\n",
        "        \"n_estimators\": [50, 75, 85, 100],\n",
        "        \"learning_rate\": [1.0, 0.5],\n",
        "        # if you want to keep depth variable, add: \"estimator__max_depth\": [2,3,4]\n",
        "    }\n",
        "else:\n",
        "    abc_tuned = AdaBoostClassifier(base_estimator=base_dt, random_state=1)\n",
        "    param_grid = {\n",
        "        \"n_estimators\": [50, 75, 85, 100],\n",
        "        \"learning_rate\": [1.0, 0.5],\n",
        "        # \"base_estimator__max_depth\": [2,3,4]\n",
        "    }\n",
        "\n",
        "# ----- randomized search (use cv=3 to keep it quick) -----\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=abc_tuned,\n",
        "    param_distributions=param_grid,\n",
        "    n_iter=8,                  # covers all combos here\n",
        "    scoring=f1_scorer,\n",
        "    cv=cv,\n",
        "    refit=True,\n",
        "    n_jobs=-1,                 # set to 1 if RAM is tight\n",
        "    verbose=1,\n",
        "    random_state=1,\n",
        "    return_train_score=False,\n",
        "    error_score=\"raise\"\n",
        ")\n",
        "\n",
        "# >>> use your ONE-HOT encoded training features <<<\n",
        "search.fit(X_train_d, y_train_1d)\n",
        "\n",
        "print(\"Best params:\", search.best_params_)\n",
        "print(\"Best CV F1: {:.4f}\".format(search.best_score_))\n",
        "\n",
        "best_ada = search.best_estimator_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eUtXWuLkVHz5",
      "metadata": {
        "id": "eUtXWuLkVHz5"
      },
      "outputs": [],
      "source": [
        "abc_tuned_score_train=model_performance_classification_sklearn(best_ada,X_train_d, y_train,)\n",
        "abc_tuned_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V3xoj-vJVLfS",
      "metadata": {
        "id": "V3xoj-vJVLfS"
      },
      "outputs": [],
      "source": [
        "abc_tuned_score_val= model_performance_classification_sklearn(best_ada,X_val_d, y_val,)\n",
        "abc_tuned_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "inK4lxCIVNZT",
      "metadata": {
        "id": "inK4lxCIVNZT"
      },
      "outputs": [],
      "source": [
        "abc_tuned_cm = confusion_matrix_sklearn(best_ada, X_val_d, y_val)\n",
        "abc_tuned_cm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "40OIeyIcYMua",
      "metadata": {
        "id": "40OIeyIcYMua"
      },
      "source": [
        "Use randomized search cv to find ideal parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cciBaj3v8dSs",
      "metadata": {
        "id": "cciBaj3v8dSs"
      },
      "source": [
        "**Best practices for hyperparameter tuning in Random Forest:**\n",
        "\n",
        "\n",
        "`n_estimators`:\n",
        "\n",
        "* Start with a specific number (50 is used in general) and increase in steps: 50, 75, 100, 125\n",
        "* Higher values generally improve performance but increase training time\n",
        "* Use 100-150 for large datasets or when variance is high\n",
        "\n",
        "\n",
        "`min_samples_leaf`:\n",
        "\n",
        "* Try values like: 1, 2, 4, 5, 10\n",
        "* Higher values reduce model complexity and help prevent overfitting\n",
        "* Use 1–2 for low-bias models, higher (like 5 or 10) for more regularized models\n",
        "* Works well in noisy datasets to smooth predictions\n",
        "\n",
        "\n",
        "`max_features`:\n",
        "\n",
        "* Try values: `\"sqrt\"` (default for classification), `\"log2\"`, `None`, or float values (e.g., `0.3`, `0.5`)\n",
        "* `\"sqrt\"` balances between diversity and performance for classification tasks\n",
        "* Lower values (e.g., `0.3`) increase tree diversity, reducing overfitting\n",
        "* Higher values (closer to `1.0`) may capture more interactions but risk overfitting\n",
        "\n",
        "\n",
        "`max_samples` (for bootstrap sampling):\n",
        "\n",
        "* Try float values between `0.5` to `1.0` or fixed integers\n",
        "* Use `0.6–0.9` to introduce randomness and reduce overfitting\n",
        "* Smaller values increase diversity between trees, improving generalization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0gIVeopgVVnq",
      "metadata": {
        "id": "0gIVeopgVVnq"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# --- labels & scorer (minority = positive) ---\n",
        "y_train_1d = np.asarray(y_train).ravel()\n",
        "pos_label = pd.Series(y_train_1d).value_counts().idxmin()\n",
        "f1_scorer = make_scorer(f1_score, pos_label=pos_label, zero_division=0)\n",
        "\n",
        "# --- model ---\n",
        "rf_tuned = RandomForestClassifier(\n",
        "    random_state=1,\n",
        "    n_jobs=-1,          # parallelize tree building     # required if you tune max_samples\n",
        ")\n",
        "\n",
        "# --- search space ---\n",
        "param_distributions = {\n",
        "    \"n_estimators\": [50, 75, 100],\n",
        "    \"min_samples_leaf\": [5, 10],\n",
        "    \"max_features\": [\"sqrt\"],        # use sqrt for speed on wide data\n",
        "    \"max_samples\": [0.5, 0.7, 1.0],  # fraction of rows per tree (needs bootstrap=True)\n",
        "    \"class_weight\": [\"balanced\", \"balanced_subsample\"],\n",
        "}\n",
        "\n",
        "# --- CV + search ---\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)  # keep 2 for speed (your choice)\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=rf_tuned,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=12,                    # small, fast sweep (adjust if you like)\n",
        "    scoring=f1_scorer,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,                    # set to 1 if RAM is tight\n",
        "    verbose=2,\n",
        "    random_state=1,\n",
        "    return_train_score=False,\n",
        "    refit=True,\n",
        "    error_score=\"raise\"\n",
        ")\n",
        "\n",
        "# >>> use your ONE-HOT encoded training matrix <<<\n",
        "search.fit(X_train_d, y_train_1d)\n",
        "\n",
        "print(\"Best params:\", search.best_params_)\n",
        "print(\"Best CV F1: {:.4f}\".format(search.best_score_))\n",
        "\n",
        "rf_tuned = search.best_estimator_   # already fitted since refit=True\n",
        "\n",
        "# If you want to evaluate:\n",
        "# from sklearn.metrics import classification_report\n",
        "# print(classification_report(y_val, rf_tuned.predict(X_val_d), digits=3, zero_division=0))\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4MeBICB8YTN4",
      "metadata": {
        "id": "4MeBICB8YTN4"
      },
      "source": [
        "Use randomized search cv to find ideal parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZM5391MQVs3c",
      "metadata": {
        "id": "ZM5391MQVs3c"
      },
      "outputs": [],
      "source": [
        "rf_tuned_score_train=model_performance_classification_sklearn(rf_tuned,X_train_d, y_train,)\n",
        "rf_tuned_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OTPlIgx4Vu6d",
      "metadata": {
        "id": "OTPlIgx4Vu6d"
      },
      "outputs": [],
      "source": [
        "rf_tuned_score_val= model_performance_classification_sklearn(rf_tuned,X_val_d, y_val,)\n",
        "rf_tuned_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "396W_4xNVxs5",
      "metadata": {
        "id": "396W_4xNVxs5"
      },
      "outputs": [],
      "source": [
        "rf_tuned_cm = confusion_matrix_sklearn(rf_tuned, X_val_d, y_val)\n",
        "rf_tuned_cm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EvIBMJe-8gEm",
      "metadata": {
        "id": "EvIBMJe-8gEm"
      },
      "source": [
        "**Best practices for hyperparameter tuning in Gradient Boosting:**\n",
        "\n",
        "`n_estimators`:\n",
        "\n",
        "* Start with 100 (default) and increase: 100, 200, 300, 500\n",
        "* Typically, higher values lead to better performance, but they also increase training time\n",
        "* Use 200–500 for larger datasets or complex problems\n",
        "* Monitor validation performance to avoid overfitting, as too many estimators can degrade generalization\n",
        "\n",
        "\n",
        "`learning_rate`:\n",
        "\n",
        "* Common values to try: 0.1, 0.05, 0.01, 0.005\n",
        "* Use lower values (e.g., 0.01 or 0.005) if you are using many estimators (e.g., > 200)\n",
        "* Higher learning rates (e.g., 0.1) can be used with fewer estimators for faster convergence\n",
        "* Always balance the learning rate with `n_estimators` to prevent overfitting or underfitting\n",
        "\n",
        "\n",
        "`subsample`:\n",
        "\n",
        "* Common values: 0.7, 0.8, 0.9, 1.0\n",
        "* Use a value between `0.7` and `0.9` for improved generalization by introducing randomness\n",
        "* `1.0` uses the full dataset for each boosting round, potentially leading to overfitting\n",
        "* Reducing `subsample` can help reduce overfitting, especially in smaller datasets\n",
        "\n",
        "\n",
        "`max_features`:\n",
        "\n",
        "* Common values: `\"sqrt\"`, `\"log2\"`, or float (e.g., `0.3`, `0.5`)\n",
        "* `\"sqrt\"` (default) works well for classification tasks\n",
        "* Lower values (e.g., `0.3`) help reduce overfitting by limiting the number of features considered at each split\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eIlMjJnWV_VM",
      "metadata": {
        "id": "eIlMjJnWV_VM"
      },
      "outputs": [],
      "source": [
        "# --- labels & scorer (minority = positive) ---\n",
        "y_train_1d = np.asarray(y_train).ravel()\n",
        "pos_label = pd.Series(y_train_1d).value_counts().idxmin()\n",
        "f1_scorer = make_scorer(f1_score, pos_label=pos_label, zero_division=0)\n",
        "\n",
        "# --- model (kept your init; note: init=None is usually faster/standard) ---\n",
        "gbc_tuned = GradientBoostingClassifier(\n",
        "    init=AdaBoostClassifier(random_state=1),\n",
        "    random_state=1\n",
        ")\n",
        "\n",
        "# --- search space (your settings) ---\n",
        "param_distributions = {\n",
        "    \"n_estimators\": [100, 200, 300, 500],\n",
        "    \"subsample\":    [0.8, 0.9, 1.0],\n",
        "    \"max_features\": [0.3, 0.5],          # fraction of features per split\n",
        "    \"learning_rate\":[0.1, 0.05, 0.01, 0.005],\n",
        "}\n",
        "\n",
        "# --- CV + randomized search ---\n",
        "cv = StratifiedKFold(n_splits=2, shuffle=True, random_state=1)\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=gbc_tuned,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,                  # covers a good portion of the 96 combos\n",
        "    scoring=f1_scorer,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,                  # parallelize across candidates/folds\n",
        "    verbose=1,\n",
        "    random_state=1,\n",
        "    return_train_score=False,\n",
        "    error_score=\"raise\",\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "# >>> use your ONE-HOT encoded training features <<<\n",
        "search.fit(X_train_d, y_train_1d)\n",
        "\n",
        "print(\"Best params:\", search.best_params_)\n",
        "print(\"Best CV F1: {:.4f}\".format(search.best_score_))\n",
        "\n",
        "gbc_tuned = search.best_estimator_   # already fit (refit=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BDn_QoXWXqGH",
      "metadata": {
        "id": "BDn_QoXWXqGH"
      },
      "outputs": [],
      "source": [
        "gbc_tuned_score_train=model_performance_classification_sklearn(gbc_tuned,X_train_d, y_train,)\n",
        "gbc_tuned_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XDeyncgJXsmx",
      "metadata": {
        "id": "XDeyncgJXsmx"
      },
      "outputs": [],
      "source": [
        "gbc_tuned_score_val= model_performance_classification_sklearn(gbc_tuned,X_val_d, y_val,)\n",
        "gbc_tuned_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FoNPz1JxXuXY",
      "metadata": {
        "id": "FoNPz1JxXuXY"
      },
      "outputs": [],
      "source": [
        "gbc_tuned_cm = confusion_matrix_sklearn(gbc_tuned, X_val_d, y_val)\n",
        "gbc_tuned_cm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "A17AkKJU8iuM",
      "metadata": {
        "id": "A17AkKJU8iuM"
      },
      "source": [
        "**Best practices for hyperparameter tuning in XGBoost:**\n",
        "\n",
        "`n_estimators`:\n",
        "\n",
        "* Start with 50 and increase in steps: 50,75,100,125.\n",
        "* Use more estimators (e.g., 150-250) when using lower learning rates\n",
        "* Monitor validation performance\n",
        "* High values improve learning but increase training time\n",
        "\n",
        "`subsample`:\n",
        "\n",
        "* Common values: 0.5, 0.7, 0.8, 1.0\n",
        "* Use `0.7–0.9` to introduce randomness and reduce overfitting\n",
        "* `1.0` uses the full dataset in each boosting round; may overfit on small datasets\n",
        "* Values < 0.5 are rarely useful unless dataset is very large\n",
        "\n",
        "`gamma`:\n",
        "\n",
        "* Try values: 0 (default), 1, 3, 5, 8\n",
        "* Controls minimum loss reduction needed for a split\n",
        "* Higher values make the algorithm more conservative (i.e., fewer splits)\n",
        "* Use values > 0 to regularize and reduce overfitting, especially on noisy data\n",
        "\n",
        "\n",
        "`colsample_bytree`:\n",
        "\n",
        "* Try values: 0.3, 0.5, 0.7, 1.0\n",
        "* Fraction of features sampled per tree\n",
        "* Lower values (e.g., 0.3 or 0.5) increase randomness and improve generalization\n",
        "* Use `1.0` when you want all features considered for every tree\n",
        "\n",
        "\n",
        "`colsample_bylevel`:\n",
        "\n",
        "* Try values: 0.3, 0.5, 0.7, 1.0\n",
        "* Fraction of features sampled at each tree level (i.e., per split depth)\n",
        "* Lower values help in regularization and reducing overfitting\n",
        "* Often used in combination with `colsample_bytree` for fine control over feature sampling\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Wyi8RcInCDDE",
      "metadata": {
        "id": "Wyi8RcInCDDE"
      },
      "outputs": [],
      "source": [
        "# --- labels & scorer (minority = positive) ---\n",
        "y_train_1d = np.asarray(y_train).ravel()\n",
        "pos_label = pd.Series(y_train_1d).value_counts().idxmin()\n",
        "f1_scorer = make_scorer(f1_score, pos_label=pos_label, zero_division=0)\n",
        "\n",
        "# --- model (use hist for speed) ---\n",
        "xgb_tuned = XGBClassifier(\n",
        "    random_state=1,\n",
        "    eval_metric='logloss',\n",
        "    tree_method='hist',\n",
        "    n_jobs=-1,\n",
        "    verbosity=0\n",
        ")\n",
        "\n",
        "# --- search space (fixed n_estimators list; your np.arange was invalid) ---\n",
        "param_distributions = {\n",
        "    \"n_estimators\": [50, 75, 100, 125],\n",
        "    \"subsample\": [0.5, 0.7, 0.8, 1.0],\n",
        "    \"gamma\": [1, 3, 5, 8],\n",
        "    \"colsample_bytree\": [0.3, 0.5, 0.7, 1.0],\n",
        "    \"colsample_bylevel\": [0.3, 0.5, 0.7, 1.0],\n",
        "    \"max_depth\": [3, 4, 5, 6],\n",
        "    \"min_child_weight\": [1, 3, 5],\n",
        "}\n",
        "\n",
        "cv = StratifiedKFold(n_splits=3, shuffle=True, random_state=1)\n",
        "\n",
        "search = RandomizedSearchCV(\n",
        "    estimator=xgb_tuned,\n",
        "    param_distributions=param_distributions,\n",
        "    n_iter=20,                 # adjust for time budget\n",
        "    scoring=f1_scorer,\n",
        "    cv=cv,\n",
        "    n_jobs=-1,                 # use all cores; set to 1 if RAM is tight\n",
        "    verbose=1,\n",
        "    random_state=1,\n",
        "    return_train_score=False,\n",
        "    error_score=\"raise\",\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "# >>> use your ONE-HOT encoded training features <<<\n",
        "search.fit(X_train_d, y_train_1d)\n",
        "\n",
        "print(\"Best params:\", search.best_params_)\n",
        "print(\"Best CV F1: {:.4f}\".format(search.best_score_))\n",
        "\n",
        "xgb_tuned = search.best_estimator_   # already fit since refit=True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vM2EsOscYpQy",
      "metadata": {
        "id": "vM2EsOscYpQy"
      },
      "outputs": [],
      "source": [
        "xgb_tuned_score_train=model_performance_classification_sklearn(xgb_tuned,X_train_d, y_train,)\n",
        "xgb_tuned_score_train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J_MzNp-eYt3X",
      "metadata": {
        "id": "J_MzNp-eYt3X"
      },
      "outputs": [],
      "source": [
        "xgb_tuned_score_val= model_performance_classification_sklearn(xgb_tuned,X_val_d, y_val,)\n",
        "xgb_tuned_score_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pj-91I19YvgK",
      "metadata": {
        "id": "Pj-91I19YvgK"
      },
      "outputs": [],
      "source": [
        "xgb_tuned_cm = confusion_matrix_sklearn(xgb_tuned, X_val_d, y_val)\n",
        "xgb_tuned_cm"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D9JNnpxa4jau",
      "metadata": {
        "id": "D9JNnpxa4jau"
      },
      "source": [
        "## Model Performance Summary and Final Model Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jYiaUm-UCB_y",
      "metadata": {
        "id": "jYiaUm-UCB_y"
      },
      "outputs": [],
      "source": [
        "# positive = minority class\n",
        "pos_label = pd.Series(np.asarray(y_train).ravel()).value_counts().idxmin()\n",
        "\n",
        "rows = []\n",
        "for name, model in models_dict.items():\n",
        "    # ensure the model is already fitted\n",
        "    y_tr_pred = model.predict(X_train_d)\n",
        "    y_te_pred = model.predict(X_test_d)\n",
        "\n",
        "    acc_tr = accuracy_score(y_train, y_tr_pred)\n",
        "    acc_te = accuracy_score(y_test,  y_te_pred)\n",
        "\n",
        "    rec_tr = recall_score(y_train, y_tr_pred, pos_label=pos_label, zero_division=0)\n",
        "    rec_te = recall_score(y_test,  y_te_pred, pos_label=pos_label, zero_division=0)\n",
        "\n",
        "    prec_tr = precision_score(y_train, y_tr_pred, pos_label=pos_label, zero_division=0)\n",
        "    prec_te = precision_score(y_test,  y_te_pred, pos_label=pos_label, zero_division=0)\n",
        "\n",
        "    f1_tr = f1_score(y_train, y_tr_pred, pos_label=pos_label, zero_division=0)\n",
        "    f1_te = f1_score(y_test,  y_te_pred, pos_label=pos_label, zero_division=0)\n",
        "\n",
        "    rows.append({\n",
        "        \"model\": name,\n",
        "        \"acc_train\": round(acc_tr, 2),\n",
        "        \"acc_test\":  round(acc_te, 2),\n",
        "        \"recall_train\": round(rec_tr, 2),\n",
        "        \"recall_test\":  round(rec_te, 2),\n",
        "        \"precision_train\": round(prec_tr, 2),\n",
        "        \"precision_test\":  round(prec_te, 2),\n",
        "        \"f1_train\": round(f1_tr, 2),\n",
        "        \"f1_test\":  round(f1_te, 2),\n",
        "    })\n",
        "\n",
        "results_df = pd.DataFrame(rows).set_index(\"model\")\n",
        "print(results_df)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mUW2opbyaPZd",
      "metadata": {
        "id": "mUW2opbyaPZd"
      },
      "source": [
        "compare the tuned models with the original models and oversampled data models (ignore undersampled)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cKyMc4xdKccc",
      "metadata": {
        "id": "cKyMc4xdKccc"
      },
      "source": [
        "* None of these models did particularly well on the f1 score.\n",
        "* gbc_tuned and xgb_tuned did very well on precision and had a 75% accuracy score.\n",
        "* I would choose the xgb_tuned model for generalizability in the f1 score and the higher accuracy result."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SluWk4OLg6YS",
      "metadata": {
        "id": "SluWk4OLg6YS"
      },
      "outputs": [],
      "source": [
        "from sklearn.inspection import permutation_importance\n",
        "\n",
        "\n",
        "# 1) Make sure these are the exact features used to train xgb_tuned\n",
        "feature_names = list(X_train_d.columns)\n",
        "\n",
        "def xgb_importances_df(model, feature_names, importance_type=\"gain\"):\n",
        "    booster = model.get_booster()\n",
        "\n",
        "    # Try the requested type first; if empty, try alternates\n",
        "    try_types = [importance_type, \"total_gain\", \"weight\", \"cover\", \"total_cover\"]\n",
        "    score = {}\n",
        "    used_type = None\n",
        "    for t in try_types:\n",
        "        score = booster.get_score(importance_type=t)\n",
        "        if score:\n",
        "            used_type = t\n",
        "            break\n",
        "\n",
        "    # Map keys (either actual names or \"fN\") to indices\n",
        "    name_to_idx = {n: i for i, n in enumerate(feature_names)}\n",
        "    imp = np.zeros(len(feature_names), dtype=float)\n",
        "\n",
        "    for k, v in score.items():\n",
        "        if k in name_to_idx:\n",
        "            imp[name_to_idx[k]] = float(v)\n",
        "        elif k.startswith(\"f\") and k[1:].isdigit():\n",
        "            idx = int(k[1:])\n",
        "            if 0 <= idx < len(imp):\n",
        "                imp[idx] = float(v)\n",
        "        # else: key we don't recognize (rare)\n",
        "\n",
        "    df = pd.DataFrame({\"feature\": feature_names, \"importance\": imp})\n",
        "    df = df.sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
        "    return df, (used_type or importance_type)\n",
        "\n",
        "imp_df, used_type = xgb_importances_df(xgb_tuned, feature_names, importance_type=\"gain\")\n",
        "print(f\"Used importance_type: {used_type}\")\n",
        "\n",
        "# If everything is zero (e.g., model super simple, or keys didn’t map), fall back to permutation importance\n",
        "if imp_df[\"importance\"].sum() == 0:\n",
        "    print(\"All zero importances from Booster; computing permutation importance on validation set...\")\n",
        "    pos_label = pd.Series(np.asarray(y_train).ravel()).value_counts().idxmin()\n",
        "    scorer = make_scorer(f1_score, pos_label=pos_label, zero_division=0)\n",
        "\n",
        "    r = permutation_importance(\n",
        "        xgb_tuned, X_val_d, y_val,\n",
        "        scoring=scorer, n_repeats=5, random_state=1, n_jobs=-1\n",
        "    )\n",
        "    imp_df = pd.DataFrame(\n",
        "        {\"feature\": feature_names, \"importance\": r.importances_mean}\n",
        "    ).sort_values(\"importance\", ascending=False).reset_index(drop=True)\n",
        "    used_type = \"permutation(F1)\"\n",
        "\n",
        "# Plot top-N\n",
        "top_n = 30\n",
        "top = imp_df.head(top_n).iloc[::-1]  # reverse for barh\n",
        "plt.figure(figsize=(8, 10))\n",
        "plt.barh(top[\"feature\"], top[\"importance\"])\n",
        "plt.title(f\"XGBoost Feature Importance — Top {top_n} ({used_type})\")\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ox4sVSyhhO10",
      "metadata": {
        "id": "Ox4sVSyhhO10"
      },
      "outputs": [],
      "source": [
        "%%time\n",
        "# Predict on the one-hot encoded TEST set (same columns/order as training)\n",
        "applicant_details = X_test_d.reindex(columns=X_train_d.columns, fill_value=0)\n",
        "\n",
        "# For a single applicant, keep 2D shape:\n",
        "# applicant_details = X_test_d.reindex(columns=X_train_d.columns, fill_value=0).iloc[[0]]\n",
        "\n",
        "print(xgb_tuned.predict(applicant_details))\n",
        "print(xgb_tuned.predict_proba(applicant_details)[:, 1])\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "congressional-knock",
      "metadata": {
        "id": "congressional-knock"
      },
      "source": [
        "## Actionable Insights and Recommendations"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KVq5cCITiXbv",
      "metadata": {
        "id": "KVq5cCITiXbv"
      },
      "source": [
        "* Use xgb_tuned for predictive capabilities.\n",
        "* It would be best to expend further time exploring feature space.\n",
        "* Time and resources were limited to the expoloration and training of certain models. Therefore, one should consider the relevance of pursuing further time and expanded feature space for more precise model development."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Y2HdXLmSJi8K",
      "metadata": {
        "id": "Y2HdXLmSJi8K"
      },
      "source": [
        "<font size=6 color='blue'>Power Ahead</font>\n",
        "___"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}